---
layout: post
title:  "Thoughts about Lasso Regression"
date:   2023-07-26
tags:   Lasso regression ML regularization
math:   true
# date:   2023-07-26 10:44:59 -0700
categories: ML
---
I'm thinking about Lasso regression, or more generally about regularization. Basically, I wanted to see if there is a nice way to rephrase Lasso regularization as a quadratic convex problem. I think the answer is 'yes, but at what cost' (pun only intended in hindsight).


## A simple realization
I don't like the absolute value function, I usually prefer to think of it as $\vert w\vert=\sqrt{w^2}$. This simple way of rewriting $\vert w\vert$ had me thinking, is it possible to write a function whose minimum is at $\sqrt{w^2}$? Well, there are many, but the one I'm going to focus on today is this:

$$
    \Lambda(s,w)=\frac{1}{2}\left(s w^2+\frac{1}{s}\right).
$$

As long as we restrict the domain to $s>0$, the minimum of $\Lambda$ w.r.t.$s$ at fixed $w$ is when $\Lambda=\vert w\vert$. Why is this the one I want to focus on? Simply because it is a quadratic function in $w$.
